---
title       : Yelp Predict Business Rating
subtitle    : 
author      : The Clown Bongo
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides

##  The
---

## Background and Models

The data comes from the Yelp Dataset Challenge. Out of the data sets, only the data on businesses and reviews for those buinesses were used.
The data was split into training and testing sets with a ratio of 60/40. 

Using the training set, classification support vector machines and random Forest models were derived. The businesses rating are from 1 to 5 with an increment of .5 that were treated as factors while building the models. 

The support vector machines model is a radial kernel C-Classification 10-fold cross-validation. The in-sample accuracy is 78.58% and in-sample error of 21.42%. The out-of sample accuracy is 78.46% and out-of-sample error of 21.54%. 

The random Forest model is a classification with 500 trees and number of variables tried at each split 3. The out-of-bag estimate is 20.12%. The in-sample accuracy is 98.43% and in-sample error of .57%. The out-of-sample accuracy is 79.14% and out-of-sample error of 20.86%.

---

## Plot of Actual Ratings Versus Predicted

The predicted rating of a business is obtained from a random Forest model.

```{r read_data_plot, echo = FALSE, fig.align = 'center'}
data_test_bus_star_predvsact_rf <- read.csv('data_test_bus_star_predvsact_rf.csv', header = TRUE, sep = ",")
plot1 <- ggplot(data_test_bus_star_predvsact_rf, aes(pred, actual)) + geom_point(aes(colour = actual, size = count)) + ggtitle("RF Pred. versus Actual Testing Set") + xlab("Predicted") + ylab("Actual")
print(plot1)
```

---

## Error Information on Testing Set

Support vector Machines
```{r read_data_svm, echo = FALSE, fig.align = 'center', warning = FALSE}
data_result_svm_test <- read.csv('table_test_svm_sensitivity_specificity.csv', header = FALSE, sep=",")
Sensitivity <- data_result_svm_test[1,2:10]
Specificity <- data_result_svm_test[2,2:10]
var_names <- c("1 Star", "1.5 Stars", "2 Stars", "2.5 Stars", "3 Stars", "3.5 Stars", "4 Stars", "4.5 Stars", "5 Stars")
table_result_svm <- data.frame(rbind(Sensitivity, Specificity))
colnames(table_result_svm) <- var_names
rownames(table_result_svm) <- c("Sensitivity", "Specificity")
print(table_result_svm)
```

Random Forest
```{r read_data_rf, echo = FALSE, fig.align = 'center', warning = FALSE}
data_result_rf_test <- read.csv('table_test_rf_sensitivity_specificity.csv', header = FALSE, sep=",")
Sensitivity <- data_result_rf_test[1,2:10]
Specificity <- data_result_rf_test[2,2:10]
var_names <- c("1 Star", "1.5 Stars", "2 Stars", "2.5 Stars", "3 Stars", "3.5 Stars", "4 Stars", "4.5 Stars", "5 Stars")
table_result_rf <- data.frame(rbind(Sensitivity, Specificity))
colnames(table_result_rf) <- var_names
rownames(table_result_rf) <- c("Sensitivity", "Specificity")
print(table_result_rf)
```

---

## Are Both Models Equivalent? Can They Be Improved?

The support vector machines and random Forest models provide pretty similar out-of sample errors, with an accuracy around 78-79%. The in-sample error for the support vector machines model is much larger than for the random Forest one but is consistent with the out-of sample error since it is a 10-fold cross-validation. By including more information like the field vote.useful and mining the text of the review for specific words like best and worst, one could weight more or less some reviews.






